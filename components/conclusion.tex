We proposed an infimal convolution-based regularization framework to address the sensitivity of graph Laplacian regularization (GLR) to edge weight uncertainties. 
By adaptively separating edge differences into an $\ell_{2}$ or an $\ell_1$ component, our method inherits the strengths of both GLR and graph total variation (GTV) while avoiding their weaknesses. 
Formulated as a convex optimization problem and solved via preconditioned primal-dual splitting, the approach achieves robust recovery over GLR and GTV under perturbed graphs, highlighting the benefit of edge-wise adaptivity. 