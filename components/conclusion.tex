We proposed an infimal convolution-based regularization framework to address the sensitivity of graph Laplacian regularization (GLR) to edge weight uncertainties. 
By adaptively separating edge differences into an $\ell_{2}$ or an $\ell_1$ component, our method inherits the strengths of both GLR and graph total variation (GTV) while avoiding their weaknesses. 
Formulated as a convex optimization problem and solved via preconditioned primal-dual splitting, the approach achieves robust recovery under perturbed graphs. 
Preliminary experiments confirmed clear improvements over GLR and GTV, highlighting the benefit of edge-wise adaptivity. 
Future work includes extending the framework to dynamic graphs and analyzing its theoretical robustness guarantees.